<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>[IJCAI-2021]A Survey of Vision-Language Pre-Trained Models&nbsp;|&nbsp;Paper Reading</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="[IJCAI-2021]A Survey of Vision-Language Pre-Trained Models">
  
    <meta name="description" content="review the progress in Vision-Language Pre-training, including modality embedding, modeling the interaction between two modality, various pre-training tasks, presentative downstream tasks">
    <meta property="og:description" content="review the progress in Vision-Language Pre-training, including modality embedding, modeling the interaction between two modality, various pre-training tasks, presentative downstream tasks">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ–ï¸&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">[IJCAI-2021]A Survey of Vision-Language Pre-Trained Models</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Fri, Nov 3, 2023</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--yellow">
            <a href="tag/Survey.html">Survey</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/274b0f259ad841b3a549e96d3e862165" class="PageRoot PageRoot--FullWidth"><h1 id="https://www.notion.so/0a05259985074c98a693cd06c4bcb4bb" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/0a05259985074c98a693cd06c4bcb4bb"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1. modality embedding</span></span></h1><div id="https://www.notion.so/8b7c462008314346bb92ea0209b3f110" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">encode images and texts into latent representations preserving their semantics.</span></span></p></div><div id="https://www.notion.so/91de3696ef80498aae92b7be619da0ec" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">design a performant architecture to model the interaction between two modalities</span></span></p></div><div id="https://www.notion.so/18dba69eff4c4885a238b78c305bc1de" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\bf{Text\ representation.}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">T</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">x</mi><mi mathvariant="bold">t</mi><mtext>Â </mtext><mi mathvariant="bold">r</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">p</mi><mi mathvariant="bold">r</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">n</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">a</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">i</mi><mi mathvariant="bold">o</mi><mi mathvariant="bold">n</mi><mi mathvariant="bold">.</mi></mrow><annotation encoding="application/x-tex">\bf{Text\ representation.}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">T</span><span class="mord mathbf">e</span><span class="mord mathbf">x</span><span class="mord mathbf">t</span><span class="mspace">Â </span><span class="mord mathbf">r</span><span class="mord mathbf">e</span><span class="mord mathbf">p</span><span class="mord mathbf">r</span><span class="mord mathbf">e</span><span class="mord mathbf">s</span><span class="mord mathbf">e</span><span class="mord mathbf">n</span><span class="mord mathbf">t</span><span class="mord mathbf">a</span><span class="mord mathbf">t</span><span class="mord mathbf">i</span><span class="mord mathbf">o</span><span class="mord mathbf">n</span><span class="mord mathbf">.</span></span></span></span></span></span></span></span><span class="SemanticString"> follow BERT to preprocess the raw text.</span></span></p></div><div id="https://www.notion.so/5f050accf6eb4ec48cd9531ec393bb53" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\bf{Image\ representation.}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">I</mi><mi mathvariant="bold">m</mi><mi mathvariant="bold">a</mi><mi mathvariant="bold">g</mi><mi mathvariant="bold">e</mi><mtext>Â </mtext><mi mathvariant="bold">r</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">p</mi><mi mathvariant="bold">r</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">n</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">a</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">i</mi><mi mathvariant="bold">o</mi><mi mathvariant="bold">n</mi><mi mathvariant="bold">.</mi></mrow><annotation encoding="application/x-tex">\bf{Image\ representation.}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">I</span><span class="mord mathbf">m</span><span class="mord mathbf">a</span><span class="mord mathbf" style="margin-right:0.01597em;">g</span><span class="mord mathbf">e</span><span class="mspace">Â </span><span class="mord mathbf">r</span><span class="mord mathbf">e</span><span class="mord mathbf">p</span><span class="mord mathbf">r</span><span class="mord mathbf">e</span><span class="mord mathbf">s</span><span class="mord mathbf">e</span><span class="mord mathbf">n</span><span class="mord mathbf">t</span><span class="mord mathbf">a</span><span class="mord mathbf">t</span><span class="mord mathbf">i</span><span class="mord mathbf">o</span><span class="mord mathbf">n</span><span class="mord mathbf">.</span></span></span></span></span></span></span></span><span class="SemanticString"> image is processed to be a sequence of embedding vectors to enable align with paired text.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/0bd9e8e9a4344fe0b28cf9e312f35371" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">There exist various ways to model visual concepts in images and how theyâ€™re represented is critical for V-L tasks.</span></span></li><li id="https://www.notion.so/315ea941131048aa94b01bdba4b0c9a9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Faster R-CNN: sequence of Region of Interest features</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/2b0c6d0af5dc4cf484484d15cce51d82" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">ViLBERT, LXMERT</span></span></li></ul></li><li id="https://www.notion.so/fdf8276f2fc545e4a940990eb0ab25c2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">ResNet: pixel-level grid features</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/b8440c7b3ea24abebc4ffd40ebf86eb6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">pixel-BERT, SOHO</span></span></li></ul></li><li id="https://www.notion.so/c6648719362742cd85965ed0cd4fc4b6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">ViT: a sequence of embeddings of image patches</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/be0d320d7b7d4e56b23e3ce8194b184c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">ALBEF, SimVLM</span></span></li></ul></li></ul><h1 id="https://www.notion.so/8fd7be9c1a96411b81aa5dc4d00e7c24" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/8fd7be9c1a96411b81aa5dc4d00e7c24"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2. interaction modeling</span></span></h1><div id="https://www.notion.so/95a587b0f3424ade81289b10f89af98d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">to aggregate information from different modalities</span></span></p></div><h2 id="https://www.notion.so/e02a597926d340cdb07b7e05c1a0020a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/e02a597926d340cdb07b7e05c1a0020a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2.1 fusion encoder: two types of fusion schemes </span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/1a7acdf6872749a7a5cee014cc622f59" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">single-stream </span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/f4a82af1f511431ba34eeff278284381" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">concatenate text embeddings and image features, and add some special embeddings to indicate position and modalities, and fed them into a transformer-based encoder</span></span></li><li id="https://www.notion.so/9f7da3e247f34ba98dde8ffaf7319c7e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">self-attention is performed directly on two modalities</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/34e06c41751740d3b35ae273f6c26232" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">assume that potential correlation and alignment can be learned by a single transformer encoder.</span></span></li></ul></li><li id="https://www.notion.so/25fcae0f7bda43c9a0f800d63223f87c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">VisualBERT, V-L BERT, OSCAR adds object tags detected from image to form a &lt;word, tag, image&gt; triple for a image-text pair.</span></span></li><li id="https://www.notion.so/a946417eba4f45acb8c7814b0b45e9b1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">intra-modality interaction may be neglected.</span></span></li></ul></li><li id="https://www.notion.so/b4e147d45d8d45b3857e1a31e9c0f735" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">dual-stream</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/3edaf3e56959470e824e73e863b65b8d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">cross-attention is performed, where query are from one modality while key and values are from another</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/9701bc1819324bc7b2f7e8aba98633c7" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">assume intra-modal interaction and cross-modal interaction need to be separated modeled</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/9736eeefbbbe4ce383a4d3600b3f0962" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">ViLBERT, LXMERT, ALBEF employs two separate transformers before cross-attention for images and texts.</span></span></li></ul></li></ul></li></ul></li><li id="https://www.notion.so/551452c73c6a49c581be1f6954dd35c3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange">good at visual understanding, but when came to Image-Text Retrieval, quite slow inferences speed.</mark></strong></span></span></li></ul><h2 id="https://www.notion.so/fdd0fdf43fe0477aa96b57a1755b8202" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/fdd0fdf43fe0477aa96b57a1755b8202"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2.2 dual encoder</span></span></h2><div id="https://www.notion.so/041bba18e97d452b932ec89dba444729" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/0e88fcbca4af4114a6c173992946c57b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">shallow attention layer or dot product for projecting image embedding and text embedding to the same semantic space for computing V-L similarity scores.</span></span></li><li id="https://www.notion.so/891273d2695a46889bd1c13aebe165b0" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">enable pre-compute and store feature vectors of images and texts</span></span></li><li id="https://www.notion.so/f899f469557b49b7af51e3aa0e117b34" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CLIP </span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/1813f3528c334d3a8b57a75efa567bcc" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange">perform good on multimodal retrieval but fail on NLVR.</mark></strong></span></span></li></ul></li></ul><h2 id="https://www.notion.so/56b662bb5f394d66a79719585d661a7a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/56b662bb5f394d66a79719585d661a7a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2.3 combination of fusion encoder and dual encoder</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/51c5557504ba464a905d558950ad4304" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">FLAVA</span></span></li><li id="https://www.notion.so/723488e9fcf5402c83d08596a66c0a3c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">VLMo introduces Mixture of Modality Expert</span></span></li></ul><h1 id="https://www.notion.so/f5f6e745686a428ebbe919be87d9d0c5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/f5f6e745686a428ebbe919be87d9d0c5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3. pre-training objectives</span></span></h1><div id="https://www.notion.so/6225802b7c084773b201718ebcd09f2b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">devise effective pre-training tasks</span></span></p></div><h2 id="https://www.notion.so/1f120f9042c4400ead69a8bda8665f2f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/1f120f9042c4400ead69a8bda8665f2f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.1 Cross-modal masked language modeling</span></span></h2><div id="https://www.notion.so/706007d2b80a401a8201e7165d78961d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fd9c13e8a-d53f-49fb-9cf5-9df918f2a3b3%2FUntitled.png?width=768&amp;table=block&amp;id=706007d2-b80a-401a-8201-e7165d78961d"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fd9c13e8a-d53f-49fb-9cf5-9df918f2a3b3%2FUntitled.png?width=768&amp;table=block&amp;id=706007d2-b80a-401a-8201-e7165d78961d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h2 id="https://www.notion.so/4ba89a452f6d43ed92e58891394ce879" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/4ba89a452f6d43ed92e58891394ce879"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.2 Cross-modal Masked Region Prediction: fine-grained to coarse-grained</span></span></h2><div id="https://www.notion.so/eed64c28fed4424fb4726973430ae777" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">learn object relationships by inferring from other unmasked regions ;</span></span></p></div><div id="https://www.notion.so/d7ac3b6e2e444b66b609be0dfa65bcbf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">learn V-L alignments by inferring from text.</span></span></p></div><h3 id="https://www.notion.so/af3ba453a15f4390862a8f8b8c976d63" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/af3ba453a15f4390862a8f8b8c976d63"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">fine-grained</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/5f37b2e3e8584d709cda9074f4f860db" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">masked region classification</strong></mark></span></span></li></ul><div id="https://www.notion.so/0dfa923cb36a4ff19790222a47266cc1" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F38c27b50-69e3-42b4-a2fc-1b475aa8de8a%2FUntitled.png?width=974&amp;table=block&amp;id=0dfa923c-b36a-4ff1-9790-222a47266cc1"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F38c27b50-69e3-42b4-a2fc-1b475aa8de8a%2FUntitled.png?width=974&amp;table=block&amp;id=0dfa923c-b36a-4ff1-9790-222a47266cc1" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/7f8d25a2677a4b11b38fdab3e073a5c4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">softmax take FC layer output as input and predict a distribution on K object classes.</span></span></p></div><div id="https://www.notion.so/9ba4f9d520424e99a9948fc07d6961f6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="c(v_{i})"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">c(v_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString"> is the true label of masked region, e.g. </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Commented">object detection output </span></span><span class="SemanticString">or re-defined visual tokens.</span></span></p><div class="Text__Children"><ul class="BulletedListWrapper"><li id="https://www.notion.so/b62a8ee63ce74eb0862c8e8cfa7e48c2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">not cross entropy, but the KL-divergence between two distributions.</span></span></li></ul></div></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/9f340178f74f42569bd03573bb6e4262" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">masked region feature regression</strong></mark></span></span></li></ul><div id="https://www.notion.so/747a2446e7654b81a00d7254039e2fe2" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fe2e046dd-b756-479c-a563-0424ebab1a49%2FUntitled.png?width=876&amp;table=block&amp;id=747a2446-e765-4b81-a00d-7254039e2fe2"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fe2e046dd-b756-479c-a563-0424ebab1a49%2FUntitled.png?width=876&amp;table=block&amp;id=747a2446-e765-4b81-a00d-7254039e2fe2" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/2d4456ce712b43a79cdce93b6413e1d1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">to reconstruct  the high-dimensional vectors instead of semantic class.</span></span></p><div class="Text__Children"><ul class="BulletedListWrapper"><li id="https://www.notion.so/87336ce0de8442ffae73f4bd7f095efa" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">visual parsing(2021): probing inter-modality of VL models, e.g. UNITER, SOHO, ViLT, proposed model in visual parsing work</span></span></li></ul></div></div><h3 id="https://www.notion.so/d5c91108a8324b71ab0399e6d07667c9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/d5c91108a8324b71ab0399e6d07667c9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">coarse-grained</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/5ee04d8797d34d17819eb539135b663d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">image-text matching</strong></mark></span></span></li></ul><div id="https://www.notion.so/0accb42640454efb817ecd1102d3a7bc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Given an image-text pair, a score function sÎ¸ measures the alignment probability between the image and text.</span></span></p></div><div id="https://www.notion.so/6e7c098e0a314cf085ad3ab9bbbf0833" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fe36d7a01-17ee-4269-bd26-be5bb4790c60%2FUntitled.png?width=825&amp;table=block&amp;id=6e7c098e-0a31-4cf0-85ad-3ab9bbbf0833"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fe36d7a01-17ee-4269-bd26-be5bb4790c60%2FUntitled.png?width=825&amp;table=block&amp;id=6e7c098e-0a31-4cf0-85ad-3ab9bbbf0833" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/18bc8d2675df4bd89e056478c6676477" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">cross-modal contrastive learning</strong></mark></span></span></li></ul><div id="https://www.notion.so/d20ca6f9e28148db81f97aa91d03e331" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F19a02718-2d44-4b03-b5ac-ccded403f856%2FUntitled.png?width=866&amp;table=block&amp;id=d20ca6f9-e281-48db-81f9-7aa91d03e331"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F19a02718-2d44-4b03-b5ac-ccded403f856%2FUntitled.png?width=866&amp;table=block&amp;id=d20ca6f9-e281-48db-81f9-7aa91d03e331" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/b50a5ecb5b3344e6a097565e782ffe64" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">sÎ¸ is a score function to justify how similar a given image-text pair is.</span></span></p></div><div id="https://www.notion.so/124ab22038c94c6b90e0ae07ea4ca32b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">in paper </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Vision-language pretraining with triple contrastive learning</strong></code></span><span class="SemanticString">, the authors claimed that this objective does not guarantee similar inputs from same modality stay close by, they thus propose to add intra-modal contrastive learning</span></span></p></div><div id="https://www.notion.so/f3937f013eee4da88f9938a79560428a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/acf5d3124c65473c9185b1b56d1819f5" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/acf5d3124c65473c9185b1b56d1819f5"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4. downstream tasks</span></span></h1><div id="https://www.notion.so/6769ed5466934b6ba3bbf9a59116ee50" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">fine-tuning on various downstream tasks</span></span></p></div><h2 id="https://www.notion.so/3d87fbad28e64484aee18e01008ae638" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/3d87fbad28e64484aee18e01008ae638"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4.1 cross-modal matching</span></span></h2><div id="https://www.notion.so/e53b9341c159437b94a8c733585f0e99" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">strategy: use pre-trained VL models as feature extractor to obtain visual representation, then learn a linear projection to predict a matching score</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/326687d646364ecaac1ce95043633f6f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Image Text Retrieval</span></span></li><li id="https://www.notion.so/a077c995dd9e455e9a44a5a592ef7703" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Visual Referring Expression</span></span></li></ul><h2 id="https://www.notion.so/e295d847c7f84b8484acbf69feab8516" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/e295d847c7f84b8484acbf69feab8516"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4.2 cross-modal reasoning</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/88a3a380545143588926e82ee502a2ec" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">VQA: predict a correct answer from pre-defined answer pool</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/ba9bbaa27d634f7b8e648ef0a1123047" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">fusion-encoder is better than dual-encoder</span></span></li></ul></li><li id="https://www.notion.so/d0e0d231670f43d49feb7216ee3ebef7" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">NLVR: a pair of images and a textual statement as input, to classify whether the statement is true or not about the image pair.</span></span></li><li id="https://www.notion.so/39bdbdd77a6040f299e0e7df277e0a63" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">VCR: decomposed into two multi-choice subtasks: 1) question answering, Qâ†’ A; 2) answer justification, Q+A â†’ R</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/2588ac4d5cd04be2a4129bb62231976c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">use a linear layer to predict a score for each possible rational option.</span></span></li></ul></li></ul><div id="https://www.notion.so/6ef766775d2f4dc3b4e515333e44684e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/b99b18fc43ca409782e7320e7eefabea" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/b99b18fc43ca409782e7320e7eefabea"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4.3 vision and language generation</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/cca2508a96614f2d9de71e61d3b9f2e9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Text-to-Image Generation</span></span></li><li id="https://www.notion.so/d82d949096be46c7845919fa61050e9a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Multimodal Text Generation</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/c18df7b0864a45fe8b63e54ffba4a98f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">condition can be text or image or their combination</span></span></li><li id="https://www.notion.so/618de7062f9d413cba811cb71bb410ac" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">image captioning</span></span></li><li id="https://www.notion.so/e404115ecc5145c1a3c655d24dd96592" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">multimodal translation </span></span></li></ul></li></ul><div id="https://www.notion.so/ae00aff001f4485092298a9575a7f729" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Paper Reading 2023</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>