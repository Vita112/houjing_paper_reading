<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>A survey on text2image diffusion models&nbsp;|&nbsp;Paper Reading</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="A survey on text2image diffusion models">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📝&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🏖️&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📝&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">A survey on text2image diffusion models</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Fri, Nov 10, 2023</span>
        
        
      </div>
    
  </header>
  <article id="https://www.notion.so/0703105fa5c14ec4a4d63bab82ebf35f" class="PageRoot PageRoot--FullWidth"><h1 id="https://www.notion.so/bda1de430abe44ba9242f43eaffe05c0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/bda1de430abe44ba9242f43eaffe05c0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Takeaway</span></span></h1><div id="https://www.notion.so/a26519fa9d44493d9b97751286c2ea6b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/3a084ac64123490ba1ad5c083a8c39f0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/3a084ac64123490ba1ad5c083a8c39f0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">What I’m thinking</span></span></h1><div id="https://www.notion.so/2774bf546b844be78604e0aaf2f64fe3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/ac2a2bae9dda4cc69254c86663b3149b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/ac2a2bae9dda4cc69254c86663b3149b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1. Background &amp;&amp; Research Problems</span></span></h1><h2 id="https://www.notion.so/16a40c019b0542f38b8a3ed25524d87a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/16a40c019b0542f38b8a3ed25524d87a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1.1 concepts</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/741b76bf9e9a4d74bd196d8e744c19de" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Diffusion Probabilistic Models(DPMs):</strong></span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/a1ab83461ef94c9ab105ca3adbbc44df" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> </strong></span><span class="SemanticString">emerges in 2015, a family of generated models that are Markov chains trained with variational inference, whose learning goal is to reserve a process of perturbing the data with noise from sample generation.</span></span></li><li id="https://www.notion.so/310a1d0e4df042b8b2e0c9619394241c" class="BulletedList"><span class="SemanticStringArray"></span></li><li id="https://www.notion.so/1e44bca23fbe4f4587d289dde0eeaaf2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">paper :Deep unsupervised learning using nonequilibrium thermodynamics </span></span></li></ul></li><li id="https://www.notion.so/d0ed3c71e47944c18a7f3749e913208e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Denoising Diffusion Probabilistic Model(DDPM): </strong></span></span></li></ul><h1 id="https://www.notion.so/c6e00c7855694392944c93f683cb6649" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/c6e00c7855694392944c93f683cb6649"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2. Method</span></span></h1><h2 id="https://www.notion.so/4d4cb0f4792f4a628dc84f398afed85a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/4d4cb0f4792f4a628dc84f398afed85a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">omitted.</span></span></h2><div id="https://www.notion.so/74825ffabaf44353bd843229ab1a546a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/995e141a8d1446f7bb1e543c838ede12" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/995e141a8d1446f7bb1e543c838ede12"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3. Multimodal guidance for image synthesis and editing</span></span></h1><ul class="BulletedListWrapper"><li id="https://www.notion.so/e742ca3ee5ce472897ed475913f631e8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgPink"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">conditional generation, unified discrete token sequence, </strong></mark></span></span></li></ul><div id="https://www.notion.so/3644fa6d6131413ba1d69ffcd9c3f06d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">create realistic images or edit real images by incorporating multimodal guidance as various conditions.</span></span></p></div><div id="https://www.notion.so/aed6fdd3c3554e8c981fe4a61ca1a14a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">there are mainly two types of guidance used in image generation and edition: intra-modal used visual clues like segmentation maps. sketch maps and so on, and inter-modal leverage texts, audios, scene graph</span></span></p></div><h2 id="https://www.notion.so/89f77ec8de1844ddb6ea8a5caeef91e7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/89f77ec8de1844ddb6ea8a5caeef91e7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.1 different guiding modalities</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/ac5370258e93412ea62bc55ab80ece25" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">visual guidance</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/dda010c080164b2fa0c1d323ab661852" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">segmentation maps, keypoints, sketch&amp;edge&amp;scribe, scene layouts, depth map, normal map, trace map</span></span></li><li id="https://www.notion.so/9148c3bf8a5c413eb841d935c42a3e80" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">due to their inherent capacity to convey spatial and structural details, they can act as a pixel-level guidance so that can be easily integrated into image generating/editing process.</span></span></li><li id="https://www.notion.so/b0b2b9c78f7647cbb92ea96340fcdbed" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">visual guidance encoding: CNN, Transformers</span></span></li></ul></li><li id="https://www.notion.so/8bfc95b1e87d431bbb0d62ffc29d6a22" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">text guidance</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/57461555fede4cbfa4e0ff1816f429ec" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">text can only convey visual concepts, can be ambiguous</span></span></li><li id="https://www.notion.so/2361e1cf33f647fc94b6eba9139dc942" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">learn an accurate and reliable mapping is difficult because of modality gap.</span></span></li><li id="https://www.notion.so/3e1baf514ba64d0c91f9265a240b0bd3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">text guidance encoding: CLIP yields informative text embeddings</span></span></li></ul></li><li id="https://www.notion.so/0d129878f9354451b9088e7351c5e933" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Audio guidance</span></span></li><li id="https://www.notion.so/8cb1bf63fab34358b687dcf7898dc1a2" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">other modality guidance</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/42193670aa2a483b99bb36a1d66ce4c8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">scene graph</span></span></li><li id="https://www.notion.so/1316b2136ff04237accd60557d35eca5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">brain signal </span></span></li><li id="https://www.notion.so/239dc94069ec4169b370960db49e2b27" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">mouse track</span></span></li></ul></li></ul><h2 id="https://www.notion.so/d31104a7d98940f0b5af3d161566aaed" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/d31104a7d98940f0b5af3d161566aaed"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.2 GAN-based approaches (some parts omitted)</span></span></h2><h3 id="https://www.notion.so/c7cc1d45c9594364be5117c5eb9e95f9" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/c7cc1d45c9594364be5117c5eb9e95f9"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">conditional GANs</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/78030b439b6c40b1abb282397493d383" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">condition the generation process on additional information, and this is achieved by feeding guidance into both generator and discriminator</span></span></li><li id="https://www.notion.so/360cd5f842fa4bce99665348dfe1008b" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">guidance is encoded into 1d or 2d features. the drawback is they struggle to capture complex scene structural relationships between guidance and real images.</span></span></li><li id="https://www.notion.so/9c2e831c5bf4475b9f3e861a3ddac203" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">in order to tackle this circumstances, using an attention module to do the alignment. </span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/1383e3e269a84e1386e4516fcf2d0306" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">by applying region-wise condition incorporation, it can direct generator’s attention to particular image regions during generation.</span></span></li><li id="https://www.notion.so/6d97b82a9bda4404bec5f9c4903ebb8e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">complex conditions can be mapped to an intermediary representation.</span></span></li></ul></li><li id="https://www.notion.so/ce457fca3edf4f9fabc25a3fafe67462" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">model architecture</span></span></li><li id="https://www.notion.so/2aa91538554f48979aedeb4278d1b2cb" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Loss design</span></span></li></ul><h3 id="https://www.notion.so/e3708f0923694a22ae1628385324b6b0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/e3708f0923694a22ae1628385324b6b0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Inversion of Unconditional GANs</span></span></h3><div id="https://www.notion.so/b69021e2810846ad847c0225e6fcd0f1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">inversion means to invert a given image back into the latent space of the GAN</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/4cae5aff54e044b7962972d84d7989ec" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">explicit cross-modal alignment</span></span></li><li id="https://www.notion.so/7c29f1cc3adc4b4bb0d679256e77151a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">implicit cross-modal supervision</span></span></li></ul><div id="https://www.notion.so/85f462d9d92f46efb67b50f5233ed45c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/3a6d2805297b47d9ad8cea03563d5851" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/3a6d2805297b47d9ad8cea03563d5851"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgPink">3.3 Diffusion-based approaches(Highlighting)</mark></strong></span></span></h2><h3 id="https://www.notion.so/94d76c0fae684c519f589c69bfccc036" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/94d76c0fae684c519f589c69bfccc036"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">扩散模型简介</span></span></h3><div id="https://www.notion.so/bd743127246141bda62f30c6312c658a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">扩散模型来源于分子热动力学中的扩散过程，即对输入</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="X_{0}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString">构建一个离散步骤的马尔科夫链，向其不断加入噪声，直至成为无法辨识的纯噪声 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="X_{T}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">X_{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> 。</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgPink">模型的学习目标是 从噪声分布出发，逐渐去除噪声  将图片还原至原始的数据分布。</mark></em></span></span></p></div><div id="https://www.notion.so/2e6e5e39cc1741988ab0e89c197ae6e3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">扩散过程分为 离散和连续两种情况，在连续情况下，可使用随机微分方程表示，并存在对应的常微分方程解；</span></span></p></div><h3 id="https://www.notion.so/8695581f63e04b198f1f77df193d9df6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/8695581f63e04b198f1f77df193d9df6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">DDPM时代的image editing，利用输入图像引导生成，无引导生成技术</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/31b9b7df57d144fe9a3eccbaa22655ec" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">前向过程的离散加噪过程 被证明为 连续时间里的随机过程的离散化形式。</span></span></li><li id="https://www.notion.so/5819156f7a474057b7003bb3a6ee005d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DDPM的优化目标—预测每一步所添加的噪声，可理解为 学习一个当前输入对目标数据分布的最优梯度方向，即学习一个数据空间上最优的梯度方向。</span></span></li><li id="https://www.notion.so/077fc986086f4f6da687c9929158384a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DDPM对图像加噪后再去噪，无法得到同一张图像，除非固定种子数。但是，通过在后向去噪过程中使用确定的常微分方程，可以得到确定性的采样结果</span></span></li><li id="https://www.notion.so/0c4a93288a92444fbe0196794c5744b0" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DDIM证明了 在前向扩散时，通过构造后向常微分方程的逆过程，可以得到前向过程的最终加噪结果。这个结论是的扩散生成变得高度可控，也催生了一系列可控图片生成工作。</span></span></li></ul><h3 id="https://www.notion.so/faa0f05e188f4a91be2c8c22217c657c" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/faa0f05e188f4a91be2c8c22217c657c"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">基于迭代去噪过程的图像编辑</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/0c3fe09deb1c4bddb8b9c13163bfa6e9" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">Conditioning Method for DDPMs</mark></span></span></li></ul><div id="https://www.notion.so/28fce2f6deac476f8f70dc7898b8e429" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">前向时信息逐渐丢失（</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">先丢失高频信息再丢失低频信息</span></span><span class="SemanticString">），后向时逐渐从噪声中补全（</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">先补全低频再补全高频</span></span><span class="SemanticString">）。</span></span></p></div><div id="https://www.notion.so/98a59cc03c3e4dfb8d7bbeec4d01c2bb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一种直觉的想法是 保存前向过程中每一步的噪声图像，然后与后向过程的噪声图像混合，这样来影响后向过程的生成结果。并且通过影响混合时注入的前向信息的多少，或者后向过程注入信息的时间步的多少，来控制生成图像与原始图像的相似程度。</span></span></p></div><div id="https://www.notion.so/fcbf1e625c2046b0b46ad5f89ef5f1ef" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过控制降维再升维的倍数来控制信息的留存比例，或者通过噪声在后向过程里添加的时间步数的多少来调整控制的强弱。</span></span></p></div><div id="https://www.notion.so/49cbcc94c4344debad3ff54e847a9ca4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">本篇文章的缺点是 只能全局修改，会保留原有图像的空间布局，无法局部调整，且无法做姿势和角度的变化</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/c7cc42c726674d809a323ca0e824afdb" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">SDEdit: guided image synthesis and editing with stochastic differential equations</mark></span></span></li></ul><div id="https://www.notion.so/94708dfca1de4cd5b820622150d4bb74" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">与上一篇文章的思想一脉相承，不同点在于本文中提出 在前向加噪过程中控制不加噪至纯噪声，而是加噪到中间过程使其保留一些低频信息。调控了信息保留的多少，实际上等价于 调控生成与原图的i相似程度。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/fe17b6994f8147bcb7d5f51cd8042e68" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">RePaint: Inpainting using denoising diffusion probabilistic models</mark></span></span></li></ul><div id="https://www.notion.so/d73d43fcc0684a65a26ed4ed7305763e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过对图像进行mask操作，将局部控制变为image inpainting（图像补全/修复）任务。想法是记录前向过程中每一步的噪声图像，在后向过程中，将unmasked区域从前向过程的记录中抽取出来，而已masked的区域则由噪声填充，这样拼合成一张完整的图像后，再开始迭代去噪。之后的每一步去噪操作都更新unmasked区域为 前向过程的记录，同时masked区域更新为后向去噪的结果。</span></span></p></div><div id="https://www.notion.so/ac628c3921fc438eb46d9849e84c110e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此处我们可以看到一个很大的弊端：掩码区域中所有信息实质上被全部丢弃了，重新生成的结果往往只在局部语义上自洽的，失去了全局语义的一致性。</span></span></p></div><div id="https://www.notion.so/2a2919eb1fb842ad8f250f403e53000f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgBlue">在论文中，作者给出了一个十分有启发性的洞见：在后向去噪时，拼合图像中包含了前向扩散过程中保存的 来自原图的静态输出，即使我们在后向去噪时不断地尝试生成 语义一致性地输出，但在下一步拼合图像时的使用的 仍旧来自原图的静态输出，导致全局语义不一致。</mark></span><span class="SemanticString">同时随着去噪过程的逐渐深入，方差逐渐减小，使得更正语义变得更为困难。</span></span></p></div><div id="https://www.notion.so/2eaa6ef23f384e439a07be8a4920e10a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">作者们提出的解决方案是在每一次去噪时，都重新将去噪后的拼合结果</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x_{t-1}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> 加一次噪声至</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x_{t}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString">,  每次迭代都重复同样的后向的去噪步骤。如此便可以得到语义一致性的输出。</span></span></p></div><div id="https://www.notion.so/c8cfd25916c84498876dc41d7e43f546" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">但是，由于其框架仍然是基于迭代去噪过程的unconditional DDPM，对掩码区域所做的调控仍旧十分有限，只能借助于随机性重新生成。</span></span></p></div><h3 id="https://www.notion.so/8a66fe6531bc4efbbcee185e5b0dc09a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/8a66fe6531bc4efbbcee185e5b0dc09a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">基于显示分类器的guided image synthesis</span></span></h3><div id="https://www.notion.so/84454f9a64464b0bbba45f252b1a1276" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">利用贝叶斯定理的思想，有条件生成的梯度 可以拆解为 一个基于显示分类器的梯度加一个常规的无条件生成的梯度， 即在训练无条件生成模型的同时，额外训练一个新的基于噪声输入的分类器。</span></span></p></div><div id="https://www.notion.so/7e7405654dad49528f3b3ade4e81307c" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F02349226-9593-44a7-b6e2-9f6c55600c5b%2FUntitled.png?width=612&amp;table=block&amp;id=7e740565-4dad-4952-8f3b-3ade4e81307c"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F02349226-9593-44a7-b6e2-9f6c55600c5b%2FUntitled.png?width=612&amp;table=block&amp;id=7e740565-4dad-4952-8f3b-3ade4e81307c" style="width:612px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/553f99137a474a90a92397076aac0556" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">Diffusion Models Beat GANs on Image Synthesis</mark></span></span></li></ul><div id="https://www.notion.so/454c297ee9764dea9bb4302777fc82fb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这篇论文有极大方面的贡献：</span></span></p><div class="Text__Children"><ol class="NumberedListWrapper"><li id="https://www.notion.so/5e149efa2c2148fd96021e030d789127" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">对UNet构架的探索：在UNet中前半段下采样的部分 添加了Att Pooling作为分类器。</span></span></li><li id="https://www.notion.so/6ae55d20a0fc48abb296a868b10e6e7f" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">显示分类器在DDPM, DDIM 的具体算法及其推导</span></span></li><li id="https://www.notion.so/8c8a53ac442f4d628be9bd272297f7d8" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">condition-guided generation的质量和多样性的权衡</span></span></li></ol></div></div><div id="https://www.notion.so/78d91a57738045f1a14d6ce8b9f47255" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这篇论文的缺点是 由于训练数据来自 DDPM前向加噪结果的增广后的数据，导致在这些噪声图像上训练后的分类判断无法复用常见的分类模型，所以需要自己训练一个新的模型，且只能按照类别生成，限制了使用场景。</span></span></p></div><h3 id="https://www.notion.so/a1fcd2e8480a45689da524a97b442a67" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a1fcd2e8480a45689da524a97b442a67"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">基于CLIP的多模态guided  image generation</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/ea8121caedb841f19937a6310652e828" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">More control for free! image synthesis with semantic diffusion guidance(SDG)</mark></span></span></li></ul><div id="https://www.notion.so/c2a4b07b63ea4f32ab7016fee12d2ab2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">最大贡献在于 扩大了</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="P(y|x)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString"> 的定义，将分类器引导拓展为 文字/图像/多模态引导，比如借助CLIP模型中文本与图像的表征相似度来计算损失。</span></span></p></div><div id="https://www.notion.so/bf5992931e7a48b4b68c36b512c49076" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">2个损失函数：一个是 考虑空间布局的对应特征图里对应位置的L2范式差，一个是考虑风格信息的对应特征图的gram-matrix。在重新训练clip模型的image encoder时，使用对比交叉熵损失。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/b4d647e3ad02445f8c99625eda0c41e5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">Blended Diffusion for Text-driven editing of natural language</mark></span></span></li></ul><div id="https://www.notion.so/4aca787168154a0984b524a74b2941d1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">与上一篇文章思想一脉相承，即在DDPM生成时做额外的梯度引导，具体地，每一步MASK区域只使用CLIP做文本引导，使得文本引导只针对某个区域更改。</span></span></p></div><div id="https://www.notion.so/5efbb466c04947a7abda983142c0c320" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">以上两种方法基于CLIP损失来引导扩散模型生成，不需要对DM做任何微调和修改。</span></span></p></div><div id="https://www.notion.so/01d075a4832b46e4b347813330dcd54d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">以下两篇则是使用CLIP损失对扩散模型进行微调。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/b9decf01310f4c0384c8b1285e5dbcae" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DiffCLIP: text-guided diffusion models for robust image manipulation</span></span></li></ul><div id="https://www.notion.so/ce38deb174494725a55c4fbad9c8a7ec" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">omitted.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/45b9a04a259049829a0408f6492b194e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">diffusion models already have a semantic latent space</span></span></li></ul><div id="https://www.notion.so/47887d30476a482abe241d2a4b024039" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">只需微调扩散模型的UNet中最中间的那一层，称之为语义潜在空间 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="h-space."><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>−</mo><mi>s</mi><mi>p</mi><mi>a</mi><mi>c</mi><mi>e</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h-space.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">p</span><span class="mord mathdefault">a</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord">.</span></span></span></span></span></span></span></p></div><div id="https://www.notion.so/f09d75d1c33e4896b82af73808b95f18" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">图像的空间布局整体语义等低频信息等 在去噪早期被决定。</span></span></span></p></div><div id="https://www.notion.so/df948ddb77cf4cbf802caaf0772a2013" class="Divider"></div><h1 id="https://www.notion.so/e20d695e403b4df0bf59bdc29e350d63" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/e20d695e403b4df0bf59bdc29e350d63"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">此处为分割线</span></span></h1><div id="https://www.notion.so/c2eebadf1bc0469ab3dca687536db27d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">以上均为 直接基于无条件DM生成的语义引导的工作，现在主流基本是有条件生成工作，这些工作都是奠基。</span></span></p></div><div id="https://www.notion.so/59d5144589c649b694d334ced88438f7" class="Divider"></div><div id="https://www.notion.so/4769948d45a0456aaf9a5153bf80384e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/529bc23294ed458e9784fad135705abd" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/529bc23294ed458e9784fad135705abd"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">基于隐式分类器的文生图大模型</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/69755272f45545a196c30ae205cffb8e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">classifier-free diffusion models</span></span></li></ul><div id="https://www.notion.so/bdd4165b2a6d4018985d81d88f07432f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">提出了一个深刻洞见：基于贝叶斯理论，将显示分类器的梯度引导再度拆解为 无条件生成的梯度预估模型（如DDPM）和 基于条件生成的梯度预估模型(条件生成建模为 UNet+cross-attention)。也可以使用同一个模型同时表示两者，只需要在生成时 设置条件向量为0或者其他。</span></span></p></div><div id="https://www.notion.so/508842283e7646209a12897b21456af4" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F66ae3854-2e33-438a-8e3e-7e52f31a7344%2FUntitled.png?width=822&amp;table=block&amp;id=50884228-3e76-4620-9a12-897b21456af4"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F66ae3854-2e33-438a-8e3e-7e52f31a7344%2FUntitled.png?width=822&amp;table=block&amp;id=50884228-3e76-4620-9a12-897b21456af4" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/6845a8da86c14750af97dee8d9e6a9b8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">classifier-free可以看作 GLIDE/Stable Diffusion/Imagen 的奠基工作之一，它使得文生图或者图生图都可以使用同一个模型，通过cross-attention注入条件信息来引导生成。</span></span></p></div><div id="https://www.notion.so/d746dd96eba8499d8ed5fede7cd9763d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/8a8aabf4b74e4d9e936d24357ae47d91" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/8a8aabf4b74e4d9e936d24357ae47d91"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">在隐式分类器上引导生成过程的 conditional generation</span></span></h3><div id="https://www.notion.so/d3cb91a2fece4454b4612dbd7f19ee56" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过对生成模型的微调，使其将目标物体的视觉信息与一个特殊字符绑定起来，然后将其当作正常语言字符使用，添加调控信息。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/70201924358a40e2ad9b3fd080077c5a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Imagic: text-based real image editing with diffusion models</span></span></li></ul><div id="https://www.notion.so/5e6d6f6035f047bdb25805c10dd06bda" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过概念绑定来实现图像编辑，具体的，对于输入图像</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></span><span class="SemanticString"> 和 描述期望生成的文本目标text_target，首先微调text_target 的文本表征，然后经过全模型微调，使得我们可以使用 微调后的text_target文本表征来生成原图，最后使用 原本的文本表征和微调后的文本表征做插值，来对原图施加影响。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/0dc03eadb2054cbf998811d641802314" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">UniTune: text-driven image editing by fine tuning an image generation model on a single image</span></span></li><li id="https://www.notion.so/7a0801c005d540e1a8d8158966174717" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DreamBooth: fine tuning text-to-image diffusion models for subject-driven generation</span></span></li></ul><div id="https://www.notion.so/7ac083aaf1354a5e80faa4cb2b78af23" class="Divider"></div><div id="https://www.notion.so/2fcf0274920b41088e435ca243393ea8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">以下是两篇无需微调的工作。</strong></span></span></p></div><div id="https://www.notion.so/93047ea638c5456aac5d33cbfc884f7c" class="Divider"></div><div id="https://www.notion.so/01c28d4a62964badb93c21db4c3ef9a3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">无需一个显示的mask，只用文本来生成mask来找到文本对应的修改位置，再使用文本调控生成方法。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/9796d8b78b63416fba84767e20eb1447" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">DiffEdit: diffusion-based semantic image editing with mask guidance</span></span></li></ul><div id="https://www.notion.so/d0de46f3489e4b07af88e8db6ea701bb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">omitted.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/951f8ca12cd84bd38962c90cb93843e5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">prompt-to-prompt image editing with cross-attention control</span></span></li></ul><div id="https://www.notion.so/713edc5300f54adc94e93d107c0ccbe2" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">作者们的方法基于一个洞见：输入的文本和像素之间存在一个空间对应关系，通过调控注意力和像素间的映射，我们能够对图像的不同区域进行准确的引导。</span></span></p></div><div id="https://www.notion.so/316916139f4140698dffe123b1d15285" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">具体的在二维权重矩阵cross-attention-map中，M_ij 表示 第j个token 对 第i个像素的权重。</span></span></p></div><div id="https://www.notion.so/fa84325cc7f94d8db18d9d9336ef6fd9" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fa925a0ae-4fcf-43cf-b666-c9632dcfdf43%2FUntitled.png?width=1440&amp;table=block&amp;id=fa84325c-c7f9-4d8d-b18d-9d9336ef6fd9"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fa925a0ae-4fcf-43cf-b666-c9632dcfdf43%2FUntitled.png?width=1440&amp;table=block&amp;id=fa84325c-c7f9-4d8d-b18d-9d9336ef6fd9" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/182a6dc33793471ca3b7e1969146a4d9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过观察上面的例子，我们发现 文本和像素组成的空间信息存在明显的对应关系，比如 bear和bird。于是这篇文章提出了三种文本引导的场景：1. 单词替换； 2. 单词增添； 3. 注意力重新加权</span></span></p></div><div id="https://www.notion.so/8428ab987a894499a3ab7cddf4b8aa35" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F8202dd05-f0ae-4505-a3a2-514cd1953efd%2FUntitled.png?width=1440&amp;table=block&amp;id=8428ab98-7a89-4499-a3ab-7cddf4b8aa35"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F8202dd05-f0ae-4505-a3a2-514cd1953efd%2FUntitled.png?width=1440&amp;table=block&amp;id=8428ab98-7a89-4499-a3ab-7cddf4b8aa35" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/1889614aaf4f4c40809d55a665cf4427" class="Divider"></div><div id="https://www.notion.so/39f9a4bd6a724ae6b43c216e0eedbbc9" class="Divider"></div><h3 id="https://www.notion.so/a4607cca692245e3a42aff20beb58c84" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a4607cca692245e3a42aff20beb58c84"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">ControlNet 如何统一多种模态引导信息？</span></span></h3><div id="https://www.notion.so/00fd64d98ad4413586f06c1425eba299" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">前提假设：在开源DM上添加各种模态引导信息 做继续训练高度可行。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/9c00ed2dbe8b43babe2b996515576298" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">如何在已有模型基础上添加可训练参数，达到在新任务上的效果迁移？</mark></span></span></li></ul><div id="https://www.notion.so/96b61397ecb74a30a29caf186e13a860" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">NLP中的方法比如，添加trainable soft prompt tokens(aka prefix tuning)，adapters等</span></span></p></div><div id="https://www.notion.so/bad01d5182b54a0b960f92bf3eba025f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ControlNet中，对于预训练好的模型的一层结构(比如SD1.5-UNet中的encoder，MidLayer的ResNet 和 Transformer layer)，固定该层参数，并将该层的输入 额外田间了一个 全连接映射后的条件c，喂入到一个和该层结构一致的复制网络里，再映射一次后，重新添加回原结构的输出。</span></span></p></div><div id="https://www.notion.so/504d1925af3f41fca192b539a1f89bd7" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fb3ee7afd-4210-42d4-a7aa-c5d4ee002660%2FUntitled.png?width=1440&amp;table=block&amp;id=504d1925-af3f-41fc-a192-b539a1f89bd7"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Fb3ee7afd-4210-42d4-a7aa-c5d4ee002660%2FUntitled.png?width=1440&amp;table=block&amp;id=504d1925-af3f-41fc-a192-b539a1f89bd7" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/92f0cb736db94f2d99e56a8c2a2d349d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/901bf0a1787e48189d042265f6512951" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">额外的引导信息如何与现有模块融合？</span></span></li></ul><div id="https://www.notion.so/4a7cfa3b4bcd421e93cedfb30b67d883" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">使用统一构架处理所有不同输入来引导生成。</span></span></p></div><div id="https://www.notion.so/c66815f5a25048bc8003442f883b86aa" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/feede66d889844fd9f8521c1198fac2d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b3d4fff2743d4735a96420c99eec88c6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/0aa4ee82fae1444d833b3c374a7cb86b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/0aa4ee82fae1444d833b3c374a7cb86b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">references:</span></span></h3><div id="https://www.notion.so/3e91b4741bbd4815b881ece20a437845" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.zhihu.com/question/568791838">(14 封私信 / 70 条消息) 2023 年扩散模型还有什么可做的方向？ - 知乎 (zhihu.com)</a></span></span></p></div><div id="https://www.notion.so/5986cd81405c4a99aadde5f9ddde122a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.zhihu.com/column/c_1416327491808608256">AIGC论文学习笔记 - 知乎 (zhihu.com)</a></span></span></p></div><div id="https://www.notion.so/d34fc2687bb04deaaeb6168b03b554bd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://zhuanlan.zhihu.com/p/605761756">ControlNet如何为扩散模型添加额外模态的引导信息 - 知乎 (zhihu.com)</a></span></span></p></div><h2 id="https://www.notion.so/b8b079f984e74e96850b75d7b41ddf02" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/b8b079f984e74e96850b75d7b41ddf02"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.4 Autoregressive approaches</span></span></h2><div id="https://www.notion.so/adb7c91e5b074c66a02478497dbeb803" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/6e2dec846749452ebbc1cae6db463604" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/54297840b9bd4007a8cf0334307f5776" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/54297840b9bd4007a8cf0334307f5776"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.5 NeRF-based approaches</span></span></h2><div id="https://www.notion.so/241eb18c687a43368063f09405da053e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/360073881d3640699d5580b677931d9d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/6d89a972c8cd4b23b3c3f99651387ff3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/af5672a98905416baeac1efe47829fbd" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/af5672a98905416baeac1efe47829fbd"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4 Datasets &amp; Evaluation Metrics</span></span></h1><div id="https://www.notion.so/bc3a1befc9d14503a6e46413045882bb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b928d81ba0b44f218219eefb47b433c5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/0538300dd40a424cb7121b227c9c90c6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/63e889380ccf4f699062665ba220d0dc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1529791f458348959b259e5902575d38" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/9f7850a73e5143e48cc65c0b286a24c6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/c2092cb44eea4120bca1a93202d08d1a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/c2092cb44eea4120bca1a93202d08d1a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">5. open challenges and discussion</span></span></h1><div id="https://www.notion.so/982759fa545d449d9a203e8996bc6ccf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1931038162e24e318ed38e2d3028b499" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/96903e9894ff4d58a4109ab1d64c3e4a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/7e2f2b07090b46b7877bde689f7117aa" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/7e2f2b07090b46b7877bde689f7117aa"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4. Discussion &amp;&amp; Limitation</span></span></h1></article>
  <footer class="Footer">
  <div>&copy; Paper Reading 2023</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>