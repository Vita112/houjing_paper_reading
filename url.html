<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>A Survey on Multimodal Large Language Models&nbsp;|&nbsp;Paper Reading</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="A Survey on Multimodal Large Language Models">
  
    <meta name="description" content="formulate MLLMs and delineate its related concepts; discussed key techniques like M-IT, M-ICT, M-CoT, LAVR.">
    <meta property="og:description" content="formulate MLLMs and delineate its related concepts; discussed key techniques like M-IT, M-ICT, M-CoT, LAVR.">
  
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ˜€&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
    <h1 class="Header__Title">A Survey on Multimodal Large Language Models</h1>
    
      <div class="DateTagBar">
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--yellow">
            <a href="tag/survey.html">survey</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/559f1d36f0c94fa9b500ab004c9a0540" class="PageRoot"><h1 id="https://www.notion.so/e980e072cc1948cfa3c422803face660" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/e980e072cc1948cfa3c422803face660"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Introducing MLLMs</span></span></h1><ul class="BulletedListWrapper"><li id="https://www.notion.so/8d3e53cdba20416292451516474d2ed4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">a MLLM refers to LLM-based model with the ability to receive and reason with multimodal information.</span></span></li></ul><h1 id="https://www.notion.so/b4a76827a4984aae9b17267888df41e0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/b4a76827a4984aae9b17267888df41e0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1. transform LLMs into multimodal chatbots or task solvers through M-IT </span></span></h1><ul class="BulletedListWrapper"><li id="https://www.notion.so/e76f53ec8f7643529cfa7cbb1af75588" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">instruction tuning: finetuning pre-trained LLMs on a collection of instruction-response datasets.</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/5b2bfd119ec44af2a0bf41424246f635" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">able to generalize to unseen tasks by following new instructions, thus boosting zero-shot performance</span></span></li></ul></li></ul><h2 id="https://www.notion.so/f56d74dfe89349b29eec7ce5ac4952a7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/f56d74dfe89349b29eec7ce5ac4952a7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1.1 adaptation to multimodality</span></span></h2><ul class="BulletedListWrapper"><li id="https://www.notion.so/81ef97ec783545cb9a6a484ece15f040" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">in order to extend instruction tuning to multimodality, the corresponding adaptations are necessary for both data and model architecture.</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/9ad323c4112149b595db699a569770c4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">reformat existing benchmark datasets into a instruction-response style</span></span></li></ul><div id="https://www.notion.so/b24b10d6d33c4a41900e12d3a8fbc0e9" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Ff81f0bf0-bb6c-4b98-bb97-e9046d085360%2FUntitled.png?width=624&amp;table=block&amp;id=b24b10d6-d33c-4a41-900e-12d3a8fbc0e9"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2Ff81f0bf0-bb6c-4b98-bb97-e9046d085360%2FUntitled.png?width=624&amp;table=block&amp;id=b24b10d6-d33c-4a41-900e-12d3a8fbc0e9" style="width:624px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/1ef6b1b461d04052af4a3c22095bdc37" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">instruction template is flexible and subject to manual designs</span></span></li></ul><div id="https://www.notion.so/62b393f3928d4c589ad0472c8ffe7a1c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h2 id="https://www.notion.so/d304c24fef3f42acba06272a9afa6c76" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/d304c24fef3f42acba06272a9afa6c76"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1.2 Data preparation</span></span></h2></li></ul><h3 id="https://www.notion.so/2bb782600ff9468491c308cb48239fd0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/2bb782600ff9468491c308cb48239fd0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">benchmark adaptation</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/4a0e22d12aee4aa0a1ed11d0c83b09e6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">hand-craft a pool of candidate instructions and sample one of them during training.</span></span></li><li id="https://www.notion.so/e4ac1f7f040b4917bafae3bc246871fe" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">manually design several seed instructions and use them to prompt GPT to generate more</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/80b63030ac7a4bb8805822d869b1de88" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">instructBLIP inherently prefer short response because it inserts explicitly short and briefly into instruction templates.</span></span></li><li id="https://www.notion.so/1b16e65618db4b838d3ca3b4862ae950" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">prompt GPT with original ones to rephrase </span></span></li></ul><h3 id="https://www.notion.so/c248890e9aa24f57b0333db9be9fc706" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/c248890e9aa24f57b0333db9be9fc706"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">self-instruction for multiple rounds of conversations</span></span></h3><div id="https://www.notion.so/b5bbd0e47eff4d9882629187765bcc55" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">some instruction-following samples are hand-crafted as seed examples, after which ChatGPT/GPT-4 is prompted to generate more instruction samples with the seed samples as guidance.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/82b6d4008ef9466eab30de6b03fd979f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">LLaVA-Instruct-150k</span></span></li></ul></li></ul><h3 id="https://www.notion.so/afbc3a2ec625422f9a71585eea6e5363" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/afbc3a2ec625422f9a71585eea6e5363"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Hybrid composition</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/6ca615dc70b941dd8d1a8be84fa9d860" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">MultiInstruct probes different strategies for training</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/5825267241b145bc9da899a7afc16d7f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">mixed instruction tuning (combine both types of data and randomly shuffle)</span></span></li><li id="https://www.notion.so/bbcb8729ac104d44aaeb7e9c3aa9377a" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">sequential instruction tuning (text data followed by multimodal data)</span></span></li><li id="https://www.notion.so/b134c58061604d519db9209215cf55c8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">and Adapter-based sequential instruction tuning</span></span></li></ul><h2 id="https://www.notion.so/545fb4960b1b46fcab20e3273c617c5d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/545fb4960b1b46fcab20e3273c617c5d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1.3 modality bridging</span></span></h2><h3 id="https://www.notion.so/7e86648be218453ba3ae9e6fa1bbb2f6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/7e86648be218453ba3ae9e6fa1bbb2f6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">learnable interface </span></span></h3><div id="https://www.notion.so/f6f9e00df54c451786f77abbacbc1b29" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">challenge is how to efficiently translate visual content into text that LLM can understand.</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/10c1f6ff34d24f458872e9ccf783298e" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">query-based approach: leverage learnable query tokens to extract information</span></span></li><li id="https://www.notion.so/4d67504769d544e2b9685ba854c7e302" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">projection-based method: add a linear embedding layer</span></span></li><li id="https://www.notion.so/67804939495847a9870390c4ad61d824" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">PEFT method: LLaMA-Adapter</span></span></li></ul><div id="https://www.notion.so/a10d302646a74decabb80c667ec9a941" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/f6014590e70d48cabce4c522d343957f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h3 id="https://www.notion.so/4b7d624679134d9c9601db97846494c2" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/4b7d624679134d9c9601db97846494c2"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">expert model such as captioning model as an expert</span></span></h3><div id="https://www.notion.so/306e408c43bb49a4b05a997ea3547006" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">convert multimodal data into languages</span></span></p></div><div id="https://www.notion.so/612e752063f24451b5c46db824bec63b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">may not as flexible as adopting a learnable interface</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/f84ce373eadc4dd782330a5822be19ab" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">VideoChat-Text</span></span></li></ul><h2 id="https://www.notion.so/892043e5244342419a8ea96344820c0e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/892043e5244342419a8ea96344820c0e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1.4 Evaluation</span></span></h2></li><li id="https://www.notion.so/c4fea87a4c47412f84bf2bee82d2420f" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">closed-set</span></span></li><li id="https://www.notion.so/de68a098fb604e3c8db6e03ec23c3097" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">open-set</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/891cdc417e1c41729323681e826a76a5" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">manual scoring</span></span></li><li id="https://www.notion.so/8640bc8c484e41ea97ab4c0328fadfb1" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">GPT-scoring</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/07655ab4192d4ee486543213163d5eaa" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">GPT-4 can only generate responses based on image-related text content, without accessing the image</span></span></li></ul></li><li id="https://www.notion.so/2f36aaa929dc41f5a8044fb78657b8d3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">sensitivity to assess the robustness to varied instructions â†’ MultiInstruct</span></span></li><li id="https://www.notion.so/04c4520f642a432998c66fd91786e9db" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">object hallucination â†’ POPE</span></span></li><li id="https://www.notion.so/92a849198ea34f19bdd7cfab2fccb9c6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">safety issues to evaluate robustness to adversarial attacks â†’On evaluating adversarial robustness</span></span></li></ul></li></ul><div id="https://www.notion.so/9b62196dbfdf4dc59b8f7b8af4874381" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p><div class="Text__Children"><div id="https://www.notion.so/9cff501996494b1f9c705bb3990e06b5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></div></div><h1 id="https://www.notion.so/aa50e2a3c75c44d2ac34eae930a8cf83" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/aa50e2a3c75c44d2ac34eae930a8cf83"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2 M-ICT</span></span></h1></article>
  <footer class="Footer">
  <div>&copy; Paper Reading 2023</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>