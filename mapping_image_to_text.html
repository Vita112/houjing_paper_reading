<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Linearly mapping from image to text space&nbsp;|&nbsp;Paper Reading</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Linearly mapping from image to text space">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📝&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📖&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;🏖️&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📝&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Linearly mapping from image to text space</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Mon, Nov 13, 2023</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--pink">
            <a href="tag/linear probing.html">linear probing</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/9e2a3b91a867444c84159fa4abcbe014" class="PageRoot PageRoot--FullWidth"><h1 id="https://www.notion.so/997585a5a5ac44f288824f0f9d0eb377" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/997585a5a5ac44f288824f0f9d0eb377"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Takeaway</span></span></h1><div id="https://www.notion.so/a02d3a0894074c28b7ecaf2f66205351" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">this paper showed that image representation can be transferred as continuous prompts to frozen LMs by training only a single linear projection.</span></span></p></div><div id="https://www.notion.so/1f8feafad6ad4c6d9724a5247d61225d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">comparable performance can be achieved on captioning and VQA, showing that LMs structurally encode conceptual information similarly to vision-based models</span></span></p></div><div id="https://www.notion.so/d43ebc208d7f44feb3e0d1fad836de0b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通过实验，本篇文章发现：1）通过训练一个简单的linear projection layer做captioning task后，vision models 学到的conceptual spaces 可以被language models理解，因而在inference 阶段可以生成caption，这表明来自两种模态的embedding space在结构上是相似的，只是相似度的精确程度取决于image encoder接受的 type of supervision。2）visual supervision only image encoder like BEIT transfers mostly coarse-grained visual properties and struggles with encouraging LM to generate exact lexical categories. 3) 预训练阶段加入linguistic supervision有助于 迁移至LM的能力。</span></span></p></div><h3 id="https://www.notion.so/77c993dbed2a4be8b362cc40d1b5aeb7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/77c993dbed2a4be8b362cc40d1b5aeb7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">reviews from openreview</span></span></h3><div id="https://www.notion.so/519ff89cad824889bb78c371d820b9a7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The authors tested vision encoders with increasing levels of linguistic supervision in pretraining,, which is reasonable and interesting.</span></span></p></div><h1 id="https://www.notion.so/24f6bce061dc4fa9ab5c6ad2ee236fd8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/24f6bce061dc4fa9ab5c6ad2ee236fd8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">What I’m thinking</span></span></h1><div id="https://www.notion.so/bda2973bf8ef4a99b4178dbafb6ee073" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">通常而言，对于一些人类通过与物理世界交互而得到的 rich conceptual knowledge，如果没有explicit grounding，LMs 不会学习到。但本文却发现只是训练一个linear transformation with captioning objective，其效果与那些 预训练或者微调vision encoder或者LM decoder的方法相当。</span></span></p></div><div id="https://www.notion.so/58dfff32334b4c1b826958b5d25931ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">个人认为实验设计很棒，使用了三种不同程度语言监督信号的视觉模型：BEIT, NFRN50, CLIP分别对应了无语言监督信号，lexical categories语言信号，caption监督信号。这样的对照实验十分直觉且自然，因为对于VL领域，经历了从pretraining-then-finetuning architecuture到 bridge-like architecture的转变，而几乎所有的bridge-like architecture都使用一个linear layer来完成将visual output embedding 映射到 language model embedding space 的过程，此外CLIP在各种CV和VL任务上的优秀表现也证明了 语言监督信号有助于得到更好的vision embeddings，基于这些背景，本文的分析有助于理解这类模型 — 将来自两个模态的frozen module通过一个linear layer桥接起来来完成VL任务。</span></span></p></div><div id="https://www.notion.so/1500f7a72c6b4f0099e0c137ac9c8872" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">文中对transfer of visual concepts的分析也十分精彩。不是对generated caption整体进行分析，而是先从ground truthcaptions中选出nouns, modifiers, relations(verbs, prepositional phrase)上，计算他们出现在generated capitons中的频率，使用F1/Recall/Precision来评估性能。此外，为了验证他们的假设，即BEIT仅仅迁移了一些对应于visual properties的coarser information，而不是lexical categories information，实验也汇报了Wu-Palmer Similarity.</span></span></p></div><div id="https://www.notion.so/29ab1d7eeb744b449502e0fc62327f6d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=29ab1d7e-eb74-4b44-9502-e0fc62327f6d"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=29ab1d7e-eb74-4b44-9502-e0fc62327f6d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/c882c1034bbd4329ad1c4648999f06fd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">（Wu-Palmer Similarity 计算了在WordNet taxonomy上 gt words 和 pred words之间的距离，显示了他们之间的语义相关性。）</span></span></p></div><div id="https://www.notion.so/ea85649ba0d94f89ab24e3b7a9b84fd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此外为了验证BEIT表现不好只是由于embedding不包含lexical concept，而不是线性迁移能力的失败，他们进行了probing实验，但这部分被放入了appendix。（是因为没那么重要吗？？）→ 参考probing visual representations</span></span></p></div><div id="https://www.notion.so/39e442163f1f4a898133604ad143ce38" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/bf850534927d4688817a5596f78090c3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/bf850534927d4688817a5596f78090c3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1. Background &amp;&amp; Research Problems</span></span></h1><div id="https://www.notion.so/eaf6798f881343829cd43481340f2bf5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">LMs can generalize to and reason about non-linguistic phenomena.</span></span></p></div><h3 id="https://www.notion.so/d25cefb2bf7f4ad2a5ad907c1c9149b0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/d25cefb2bf7f4ad2a5ad907c1c9149b0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">related work:</span></span></h3><div id="https://www.notion.so/58088d054bd14d04bf78638960bb5fa1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">visual inputs → VLs → described in text as text prompts →  LMs</span></span></p></div><div id="https://www.notion.so/54ec8e0044c94cc5a62eaa115f1287bc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">model “stitching” :  two different models are attached at a certain layer.</span></span></p></div><h3 id="https://www.notion.so/a74242b64dde45e9af57c2986d13ef1f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a74242b64dde45e9af57c2986d13ef1f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">research problems:</span></span></h3><div id="https://www.notion.so/f28fb123b7eb4a2eac996cf9aa89594f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">to what extent (if at all) a language model trained on text-only data can learn aspects of the physical world. </span></span></p></div><div id="https://www.notion.so/57b0b8451c2f4fcd9007a7704d0f8843" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">if conceptual representation spaces from two modalities are structured similarly.</span></span></p></div><h1 id="https://www.notion.so/3d1a38c1881b4d1e9a907e279904a77e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/3d1a38c1881b4d1e9a907e279904a77e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2. Method</span></span></h1><div id="https://www.notion.so/ca6aaefb9b1240a2a5b55678ebcfe09e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">linearly transform an image representation into “soft prompts” — vectors in embedding space that don’t correspond to discrete language tokens.</span></span></p></div><div id="https://www.notion.so/7d50c733be534c99b87be974baefbdd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">(train a single</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"> linear layer </span></em></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">P</strong></span></em></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"> to project</span></em></span><span class="SemanticString"> from the hidden size </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">h_I </em></span><span class="SemanticString">of a pretrained image encoder</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> into</em></span><span class="SemanticString"> the input space</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> e_L</em></span><span class="SemanticString"> of a generative language model</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> for an image captioning task.</em></span></span></p></div><div id="https://www.notion.so/54f1b92b4a1c4347bbb1c60c18af5acf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">weights of linear projection are tuned for image captioning.</span></span></p></div><div id="https://www.notion.so/b8c91ba0168e458ab565345163a00895" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F612bfe27-ff82-4c54-bdb6-04e4a882940e%2FUntitled.png?width=2218&amp;table=block&amp;id=b8c91ba0-168e-458a-b565-345163a00895"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F612bfe27-ff82-4c54-bdb6-04e4a882940e%2FUntitled.png?width=2218&amp;table=block&amp;id=b8c91ba0-168e-458a-b565-345163a00895" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/6586ebd2311f4e459be7e78fa4434739" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">by using three vision encoders with different levels of linguistic supervision to train diff projections.</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/e45877617e524e49ac9bf454b76f8c06" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">BEIT</span></span></li><li id="https://www.notion.so/631698fed5604e41b412cc664c058074" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Normalizer Free Resnet50</span></span></li><li id="https://www.notion.so/0216c71bfa97489db4ea39da16063ace" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CLIP</span></span></li></ul></li></ul><h1 id="https://www.notion.so/e1af214e3fa44a8c929d54cb6d484ddc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/e1af214e3fa44a8c929d54cb6d484ddc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3. Experiments</span></span></h1><div id="https://www.notion.so/a78b9624292a407ba0a39f15067ae0ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">project visual embeddings from different vision encoder to a </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="e_{L} *k"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub><mo>∗</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">e_{L} *k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span></span></span><span class="SemanticString"> sequence of soft prompts.  </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="e_{L}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">e_{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> =4096  indicate input space of LM, k is determined by architecture of vision encoder E.</span></span></p></div><div id="https://www.notion.so/f2ff05d6280043008106d3de9bec0538" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">following MAGMA and Frozen, train this projection layer on captioning task  with same basic hyperparameters and setting as that in MAGMA paper.</span></span></p></div><h3 id="https://www.notion.so/4acbedb39dfc4a88877f89c8ca30150e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/4acbedb39dfc4a88877f89c8ca30150e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">performance on VL tasks</span></span></h3><div id="https://www.notion.so/5ff21340e24f4600ac908b8814e70700" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F0e5556e6-281f-49a9-b3e0-c5ce5c566829%2FUntitled.png?width=1154&amp;table=block&amp;id=5ff21340-e24f-4600-ac90-8b8814e70700"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F0e5556e6-281f-49a9-b3e0-c5ce5c566829%2FUntitled.png?width=1154&amp;table=block&amp;id=5ff21340-e24f-4600-ac90-8b8814e70700" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/cfc2f3ed08b341ac8585a095580af9f6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">BEIT 和 BEIT FT. 的性能差异 strengthen the claim that it is the linguistic supervision of the pretraining task that plays a big role in performance, not the architecture.</span></span></li></ul><div id="https://www.notion.so/7c2ee803c36c4c39a6c950042aeabccd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">除了在常规的evaluation metrics上进行实验分析外，还从coarse-grained和finegrained两方面验证了 经过linear layer映射后的visual embedding 的迁移至LM的能力。此外， 为了探索来自三个代表性vision encoder的visual embedding，使用了linear probes来分析</span></span></p></div><h3 id="https://www.notion.so/c60df9d590d14537bbf4afe39c1bb4dc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/c60df9d590d14537bbf4afe39c1bb4dc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Transfer of visual concepts</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/d67d5746f627446390a91f6c078a0fe4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">transfer of lexical categorical concepts</strong></mark></span></span></li></ul><div id="https://www.notion.so/3a4fda7fe4ad47a8ae4cb1888a75578e" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=3a4fda7f-e4ad-47a8-ae4c-b1888a75578e"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=3a4fda7f-e4ad-47a8-ae4c-b1888a75578e" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/d3a62876581d456fa68d7aa384330072" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">probing visual representation</strong></mark></span></span></li></ul><div id="https://www.notion.so/71ad5ce3beab4e34a5ccc632a5dd31b4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">setting: train a single linear layer as probe taking as input image encodings, then project to the number of classes</strong></span><span class="SemanticString"> to classify fine-grained lexical and coarse-grained categorical concepts on datasets like COCO, CC3M, CIFAR-100. </span></span></p><div class="Text__Children"><ul class="BulletedListWrapper"><li id="https://www.notion.so/fa787c42baa8465880bdaa5234e31ac8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">results of coarse-grained supercategory probes &amp; fine-grained object label probe</span></span></li></ul><div id="https://www.notion.so/3b73f529550b48b3b8391b25e291acee" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F2c6ecf97-2ee2-44a3-a7e5-42c8c9a81983%2FUntitled.png?width=1165&amp;table=block&amp;id=3b73f529-550b-48b3-b839-1b25e291acee"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F2c6ecf97-2ee2-44a3-a7e5-42c8c9a81983%2FUntitled.png?width=1165&amp;table=block&amp;id=3b73f529-550b-48b3-b839-1b25e291acee" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/69845d29dd254574871989a547b1cde3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">transfer of coarse-grained perceptual concepts</strong></mark></span></span></li></ul><div id="https://www.notion.so/68d441f8af774b6dab9bd4545477c108" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/d674bed551dd4db5880c6c892fe753cf" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/d674bed551dd4db5880c6c892fe753cf"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4. Discussion &amp;&amp; Limitation</span></span></h1></article>
  <footer class="Footer">
  <div>&copy; Paper Reading 2023</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>