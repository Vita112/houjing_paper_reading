<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ğŸ“–&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Linearly mapping from image to text space&nbsp;|&nbsp;Paper Reading</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Linearly mapping from image to text space">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ğŸ“&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ğŸ“–&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ğŸ–ï¸&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ğŸ“&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">Linearly mapping from image to text space</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Mon, Nov 13, 2023</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--pink">
            <a href="tag/linear probing.html">linear probing</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/9e2a3b91a867444c84159fa4abcbe014" class="PageRoot PageRoot--FullWidth"><h1 id="https://www.notion.so/997585a5a5ac44f288824f0f9d0eb377" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/997585a5a5ac44f288824f0f9d0eb377"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Takeaway</span></span></h1><div id="https://www.notion.so/a02d3a0894074c28b7ecaf2f66205351" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">this paper showed that image representation can be transferred as continuous prompts to frozen LMs by training only a single linear projection.</span></span></p></div><div id="https://www.notion.so/1f8feafad6ad4c6d9724a5247d61225d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">comparable performance can be achieved on captioning and VQA, showing that LMs structurally encode conceptual information similarly to vision-based models</span></span></p></div><div id="https://www.notion.so/d43ebc208d7f44feb3e0d1fad836de0b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">é€šè¿‡å®éªŒï¼Œæœ¬ç¯‡æ–‡ç« å‘ç°ï¼š1ï¼‰é€šè¿‡è®­ç»ƒä¸€ä¸ªç®€å•çš„linear projection layeråšcaptioning taskåï¼Œvision models å­¦åˆ°çš„conceptual spaces å¯ä»¥è¢«language modelsç†è§£ï¼Œå› è€Œåœ¨inference é˜¶æ®µå¯ä»¥ç”Ÿæˆcaptionï¼Œè¿™è¡¨æ˜æ¥è‡ªä¸¤ç§æ¨¡æ€çš„embedding spaceåœ¨ç»“æ„ä¸Šæ˜¯ç›¸ä¼¼çš„ï¼Œåªæ˜¯ç›¸ä¼¼åº¦çš„ç²¾ç¡®ç¨‹åº¦å–å†³äºimage encoderæ¥å—çš„ type of supervisionã€‚2ï¼‰visual supervision only image encoder like BEIT transfers mostly coarse-grained visual properties and struggles with encouraging LM to generate exact lexical categories. 3) é¢„è®­ç»ƒé˜¶æ®µåŠ å…¥linguistic supervisionæœ‰åŠ©äº è¿ç§»è‡³LMçš„èƒ½åŠ›ã€‚</span></span></p></div><h3 id="https://www.notion.so/77c993dbed2a4be8b362cc40d1b5aeb7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/77c993dbed2a4be8b362cc40d1b5aeb7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">reviews from openreview</span></span></h3><div id="https://www.notion.so/519ff89cad824889bb78c371d820b9a7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The authors tested vision encoders with increasing levels of linguistic supervision in pretraining,, which is reasonable and interesting.</span></span></p></div><h1 id="https://www.notion.so/24f6bce061dc4fa9ab5c6ad2ee236fd8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/24f6bce061dc4fa9ab5c6ad2ee236fd8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">What Iâ€™m thinking</span></span></h1><div id="https://www.notion.so/bda2973bf8ef4a99b4178dbafb6ee073" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">é€šå¸¸è€Œè¨€ï¼Œå¯¹äºä¸€äº›äººç±»é€šè¿‡ä¸ç‰©ç†ä¸–ç•Œäº¤äº’è€Œå¾—åˆ°çš„ rich conceptual knowledgeï¼Œå¦‚æœæ²¡æœ‰explicit groundingï¼ŒLMs ä¸ä¼šå­¦ä¹ åˆ°ã€‚ä½†æœ¬æ–‡å´å‘ç°åªæ˜¯è®­ç»ƒä¸€ä¸ªlinear transformation with captioning objectiveï¼Œå…¶æ•ˆæœä¸é‚£äº› é¢„è®­ç»ƒæˆ–è€…å¾®è°ƒvision encoderæˆ–è€…LM decoderçš„æ–¹æ³•ç›¸å½“ã€‚</span></span></p></div><div id="https://www.notion.so/58dfff32334b4c1b826958b5d25931ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ä¸ªäººè®¤ä¸ºå®éªŒè®¾è®¡å¾ˆæ£’ï¼Œä½¿ç”¨äº†ä¸‰ç§ä¸åŒç¨‹åº¦è¯­è¨€ç›‘ç£ä¿¡å·çš„è§†è§‰æ¨¡å‹ï¼šBEIT, NFRN50, CLIPåˆ†åˆ«å¯¹åº”äº†æ— è¯­è¨€ç›‘ç£ä¿¡å·ï¼Œlexical categoriesè¯­è¨€ä¿¡å·ï¼Œcaptionç›‘ç£ä¿¡å·ã€‚è¿™æ ·çš„å¯¹ç…§å®éªŒååˆ†ç›´è§‰ä¸”è‡ªç„¶ï¼Œå› ä¸ºå¯¹äºVLé¢†åŸŸï¼Œç»å†äº†ä»pretraining-then-finetuning architecutureåˆ° bridge-like architectureçš„è½¬å˜ï¼Œè€Œå‡ ä¹æ‰€æœ‰çš„bridge-like architectureéƒ½ä½¿ç”¨ä¸€ä¸ªlinear layeræ¥å®Œæˆå°†visual output embedding æ˜ å°„åˆ° language model embedding space çš„è¿‡ç¨‹ï¼Œæ­¤å¤–CLIPåœ¨å„ç§CVå’ŒVLä»»åŠ¡ä¸Šçš„ä¼˜ç§€è¡¨ç°ä¹Ÿè¯æ˜äº† è¯­è¨€ç›‘ç£ä¿¡å·æœ‰åŠ©äºå¾—åˆ°æ›´å¥½çš„vision embeddingsï¼ŒåŸºäºè¿™äº›èƒŒæ™¯ï¼Œæœ¬æ–‡çš„åˆ†ææœ‰åŠ©äºç†è§£è¿™ç±»æ¨¡å‹ â€” å°†æ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„frozen moduleé€šè¿‡ä¸€ä¸ªlinear layeræ¡¥æ¥èµ·æ¥æ¥å®ŒæˆVLä»»åŠ¡ã€‚</span></span></p></div><div id="https://www.notion.so/1500f7a72c6b4f0099e0c137ac9c8872" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">æ–‡ä¸­å¯¹transfer of visual conceptsçš„åˆ†æä¹Ÿååˆ†ç²¾å½©ã€‚ä¸æ˜¯å¯¹generated captionæ•´ä½“è¿›è¡Œåˆ†æï¼Œè€Œæ˜¯å…ˆä»ground truthcaptionsä¸­é€‰å‡ºnouns, modifiers, relations(verbs, prepositional phrase)ä¸Šï¼Œè®¡ç®—ä»–ä»¬å‡ºç°åœ¨generated capitonsä¸­çš„é¢‘ç‡ï¼Œä½¿ç”¨F1/Recall/Precisionæ¥è¯„ä¼°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†éªŒè¯ä»–ä»¬çš„å‡è®¾ï¼Œå³BEITä»…ä»…è¿ç§»äº†ä¸€äº›å¯¹åº”äºvisual propertiesçš„coarser informationï¼Œè€Œä¸æ˜¯lexical categories informationï¼Œå®éªŒä¹Ÿæ±‡æŠ¥äº†Wu-Palmer Similarity.</span></span></p></div><div id="https://www.notion.so/29ab1d7eeb744b449502e0fc62327f6d" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=29ab1d7e-eb74-4b44-9502-e0fc62327f6d"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=29ab1d7e-eb74-4b44-9502-e0fc62327f6d" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/c882c1034bbd4329ad1c4648999f06fd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">ï¼ˆWu-Palmer Similarity è®¡ç®—äº†åœ¨WordNet taxonomyä¸Š gt words å’Œ pred wordsä¹‹é—´çš„è·ç¦»ï¼Œæ˜¾ç¤ºäº†ä»–ä»¬ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚ï¼‰</span></span></p></div><div id="https://www.notion.so/ea85649ba0d94f89ab24e3b7a9b84fd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">æ­¤å¤–ä¸ºäº†éªŒè¯BEITè¡¨ç°ä¸å¥½åªæ˜¯ç”±äºembeddingä¸åŒ…å«lexical conceptï¼Œè€Œä¸æ˜¯çº¿æ€§è¿ç§»èƒ½åŠ›çš„å¤±è´¥ï¼Œä»–ä»¬è¿›è¡Œäº†probingå®éªŒï¼Œä½†è¿™éƒ¨åˆ†è¢«æ”¾å…¥äº†appendixã€‚ï¼ˆæ˜¯å› ä¸ºæ²¡é‚£ä¹ˆé‡è¦å—ï¼Ÿï¼Ÿï¼‰â†’ å‚è€ƒprobing visual representations</span></span></p></div><div id="https://www.notion.so/39e442163f1f4a898133604ad143ce38" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/bf850534927d4688817a5596f78090c3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/bf850534927d4688817a5596f78090c3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1. Background &amp;&amp; Research Problems</span></span></h1><div id="https://www.notion.so/eaf6798f881343829cd43481340f2bf5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">LMs can generalize to and reason about non-linguistic phenomena.</span></span></p></div><h3 id="https://www.notion.so/d25cefb2bf7f4ad2a5ad907c1c9149b0" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/d25cefb2bf7f4ad2a5ad907c1c9149b0"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">related work:</span></span></h3><div id="https://www.notion.so/58088d054bd14d04bf78638960bb5fa1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">visual inputs â†’ VLs â†’ described in text as text prompts â†’  LMs</span></span></p></div><div id="https://www.notion.so/54ec8e0044c94cc5a62eaa115f1287bc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">model â€œstitchingâ€ :  two different models are attached at a certain layer.</span></span></p></div><h3 id="https://www.notion.so/a74242b64dde45e9af57c2986d13ef1f" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a74242b64dde45e9af57c2986d13ef1f"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">research problems:</span></span></h3><div id="https://www.notion.so/f28fb123b7eb4a2eac996cf9aa89594f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">to what extent (if at all) a language model trained on text-only data can learn aspects of the physical world. </span></span></p></div><div id="https://www.notion.so/57b0b8451c2f4fcd9007a7704d0f8843" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">if conceptual representation spaces from two modalities are structured similarly.</span></span></p></div><h1 id="https://www.notion.so/3d1a38c1881b4d1e9a907e279904a77e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/3d1a38c1881b4d1e9a907e279904a77e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2. Method</span></span></h1><div id="https://www.notion.so/ca6aaefb9b1240a2a5b55678ebcfe09e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">linearly transform an image representation into â€œsoft promptsâ€ â€” vectors in embedding space that donâ€™t correspond to discrete language tokens.</span></span></p></div><div id="https://www.notion.so/7d50c733be534c99b87be974baefbdd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">(train a single</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"> linear layer </span></em></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">P</strong></span></em></span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown"> to project</span></em></span><span class="SemanticString"> from the hidden size </span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">h_I </em></span><span class="SemanticString">of a pretrained image encoder</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> into</em></span><span class="SemanticString"> the input space</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> e_L</em></span><span class="SemanticString"> of a generative language model</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic"> for an image captioning task.</em></span></span></p></div><div id="https://www.notion.so/54f1b92b4a1c4347bbb1c60c18af5acf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">weights of linear projection are tuned for image captioning.</span></span></p></div><div id="https://www.notion.so/b8c91ba0168e458ab565345163a00895" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F612bfe27-ff82-4c54-bdb6-04e4a882940e%2FUntitled.png?width=2218&amp;table=block&amp;id=b8c91ba0-168e-458a-b565-345163a00895"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F612bfe27-ff82-4c54-bdb6-04e4a882940e%2FUntitled.png?width=2218&amp;table=block&amp;id=b8c91ba0-168e-458a-b565-345163a00895" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/6586ebd2311f4e459be7e78fa4434739" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">by using three vision encoders with different levels of linguistic supervision to train diff projections.</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/e45877617e524e49ac9bf454b76f8c06" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">BEIT</span></span></li><li id="https://www.notion.so/631698fed5604e41b412cc664c058074" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Normalizer Free Resnet50</span></span></li><li id="https://www.notion.so/0216c71bfa97489db4ea39da16063ace" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">CLIP</span></span></li></ul></li></ul><h1 id="https://www.notion.so/e1af214e3fa44a8c929d54cb6d484ddc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/e1af214e3fa44a8c929d54cb6d484ddc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3. Experiments</span></span></h1><div id="https://www.notion.so/a78b9624292a407ba0a39f15067ae0ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">project visual embeddings from different vision encoder to a </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="e_{L} *k"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub><mo>âˆ—</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">e_{L} *k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61528em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span></span></span><span class="SemanticString"> sequence of soft prompts.  </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="e_{L}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">e_{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> =4096  indicate input space of LM, k is determined by architecture of vision encoder E.</span></span></p></div><div id="https://www.notion.so/f2ff05d6280043008106d3de9bec0538" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">following MAGMA and Frozen, train this projection layer on captioning task  with same basic hyperparameters and setting as that in MAGMA paper.</span></span></p></div><h3 id="https://www.notion.so/4acbedb39dfc4a88877f89c8ca30150e" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/4acbedb39dfc4a88877f89c8ca30150e"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">performance on VL tasks</span></span></h3><div id="https://www.notion.so/5ff21340e24f4600ac908b8814e70700" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F0e5556e6-281f-49a9-b3e0-c5ce5c566829%2FUntitled.png?width=1154&amp;table=block&amp;id=5ff21340-e24f-4600-ac90-8b8814e70700"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F0e5556e6-281f-49a9-b3e0-c5ce5c566829%2FUntitled.png?width=1154&amp;table=block&amp;id=5ff21340-e24f-4600-ac90-8b8814e70700" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/cfc2f3ed08b341ac8585a095580af9f6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">BEIT å’Œ BEIT FT. çš„æ€§èƒ½å·®å¼‚ strengthen the claim that it is the linguistic supervision of the pretraining task that plays a big role in performance, not the architecture.</span></span></li></ul><div id="https://www.notion.so/7c2ee803c36c4c39a6c950042aeabccd" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">é™¤äº†åœ¨å¸¸è§„çš„evaluation metricsä¸Šè¿›è¡Œå®éªŒåˆ†æå¤–ï¼Œè¿˜ä»coarse-grainedå’Œfinegrainedä¸¤æ–¹é¢éªŒè¯äº† ç»è¿‡linear layeræ˜ å°„åçš„visual embedding çš„è¿ç§»è‡³LMçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œ ä¸ºäº†æ¢ç´¢æ¥è‡ªä¸‰ä¸ªä»£è¡¨æ€§vision encoderçš„visual embeddingï¼Œä½¿ç”¨äº†linear probesæ¥åˆ†æ</span></span></p></div><h3 id="https://www.notion.so/c60df9d590d14537bbf4afe39c1bb4dc" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/c60df9d590d14537bbf4afe39c1bb4dc"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Transfer of visual concepts</span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/d67d5746f627446390a91f6c078a0fe4" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">transfer of lexical categorical concepts</strong></mark></span></span></li></ul><div id="https://www.notion.so/3a4fda7fe4ad47a8ae4cb1888a75578e" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=3a4fda7f-e4ad-47a8-ae4c-b1888a75578e"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F4e2116e0-37e3-441b-86d1-75e92bb9c2ce%2FUntitled.png?width=1186&amp;table=block&amp;id=3a4fda7f-e4ad-47a8-ae4c-b1888a75578e" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/d3a62876581d456fa68d7aa384330072" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">probing visual representation</strong></mark></span></span></li></ul><div id="https://www.notion.so/71ad5ce3beab4e34a5ccc632a5dd31b4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">setting: train a single linear layer as probe taking as input image encodings, then project to the number of classes</strong></span><span class="SemanticString"> to classify fine-grained lexical and coarse-grained categorical concepts on datasets like COCO, CC3M, CIFAR-100. </span></span></p><div class="Text__Children"><ul class="BulletedListWrapper"><li id="https://www.notion.so/fa787c42baa8465880bdaa5234e31ac8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">results of coarse-grained supercategory probes &amp; fine-grained object label probe</span></span></li></ul><div id="https://www.notion.so/3b73f529550b48b3b8391b25e291acee" class="Image Image--PageWidth"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F2c6ecf97-2ee2-44a3-a7e5-42c8c9a81983%2FUntitled.png?width=1165&amp;table=block&amp;id=3b73f529-550b-48b3-b839-1b25e291acee"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2f766b29-2ea4-4bd3-b5a4-e00aa9e137a5%2F2c6ecf97-2ee2-44a3-a7e5-42c8c9a81983%2FUntitled.png?width=1165&amp;table=block&amp;id=3b73f529-550b-48b3-b839-1b25e291acee" style="width:100%"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/69845d29dd254574871989a547b1cde3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgOrange"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">transfer of coarse-grained perceptual concepts</strong></mark></span></span></li></ul><div id="https://www.notion.so/68d441f8af774b6dab9bd4545477c108" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/d674bed551dd4db5880c6c892fe753cf" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/d674bed551dd4db5880c6c892fe753cf"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">4. Discussion &amp;&amp; Limitation</span></span></h1></article>
  <footer class="Footer">
  <div>&copy; Paper Reading 2023</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>